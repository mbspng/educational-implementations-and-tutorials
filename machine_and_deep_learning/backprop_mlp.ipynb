{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation for educational purposes of an MLP and backpropagation with a basic optimization rule. The frist few cells derive the parameter updates formally. Then the implementation follows that uses these equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of loss function w. r. t. the activation of the output layer:\n",
    "\n",
    "Let the output layer activation be the sigmoid function and let the loss function be the mean square error.\\\\\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial l(y, f(x))}{\\partial A^{(L)}_i} =& \\frac{\\partial l(y, f(x))}{\\partial f (x)_i}\\\\\n",
    "    & \\text{by definition}\\\\\n",
    "    =& \\frac{\\partial}{\\partial f(x)}\\frac{1}{2n} \\sum_{j=1}^n \\left(y_j - f(x)_j \\right)^2\\\\\n",
    "    =& \\frac{1}{2n} \\sum_{j=1}^n \\frac{\\partial \\left(y_j - f(x)_j^2\\right)}{\\partial f(x)_i}\\\\\n",
    "    =& \\frac{1}{2n} 2 \\left(y_i - f(x)_i\\right)\\\\\n",
    "    =& \\frac{1}{n} (y_i - f(x)_i)\\\\ \n",
    "    \\propto{}& y_i - f(x)_i\\label{eq1}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{A^{(L)}}{l(y, f(x))} \\propto y - f(x)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $f$ is the function computed by the network and $g^{(L)}$ is the activation function of layer $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of loss function, w. r. t. the pre-activation of the output layer:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial P^{(L)}_i} ={}& \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial A^{(L)}_i} \\frac{\\partial A^{(L)}_i}{\\partial P^{(L)}_i}\\\\\n",
    "    \\propto & \\left(y_i - f(x)_i\\right) \\frac{\\partial A^{(L)}_i}{\\partial P^{(L)}_i}\\\\\n",
    "    =& \\left(y_i - f(x)_i\\right) \\frac{\\partial g^{(L)}\\left(P^{(L)}\\right)_i}{\\partial P^{(L)}_i}\\\\\n",
    "    =& \\left(y_i - f(x)_i\\right) g'^{(L)}\\left(P^{(L)}\\right)_i\\\\\n",
    "\\end{align*}\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{P^{(L)}} l\\left(y, f(x)\\right) \\propto \\left(y - f(x)\\right) \\otimes {g'}^{(L)}(P^{(L)})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of loss function w. r. t. the activation of a hidden layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "    P^{(L+1)}_i =  B^{(L+1)}_i + \\sum_j W_{ij}^{(L+1)} A^{(L)}_j\n",
    "\\end{equation*}\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial A^{(L)}_i} =& \\sum_i \\frac{\\partial \\left((y, f(x)\\right)}{\\partial P^{(L+1)}_i} \\frac{\\partial P^{(L+1)}_i}{\\partial A^{(L)}_i}\\\\\n",
    "    =& \\sum_i \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial P^{(L+1)}_i} W_{ij}^{(L+1)}\\\\\n",
    "    =& \\left( W^{(L+1)}_{\\cdot j} \\right)^\\top  \\nabla_{P^{(L+1)}} l\\left(y, f(x)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{A^{(L)}} l\\left(y, f(x)\\right) = \\left( W^{(L+1)} \\right)^\\top  \\nabla_{P^{(L+1)}} l\\left(y, f(x)\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of loss function w. r. t. the pre-activation of a hidden layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "    A^{(L)}_i =  g^{(L)} \\left( P^{(L)} \\right)_i\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial P^{(L)}_i} =& \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial A^{(L)}_i} \\frac{\\partial A^{(L)}_i}{\\partial P^{(L)}_i}\\\\\n",
    "    =& \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial A^{(L)}_i} g'^{(L)} \\left( P^{(L)} \\right)_i\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{P^{(L)}} l\\left(y, f(x)\\right) = \\left(\\nabla_{A^{(L)}} l\\left(y, f(x)\\right)\\right)^\\top \\underbrace{\\nabla_{P^{(L)}} A^{(L)}}_{\\text{Jacobian}}\n",
    "\\end{equation*}\n",
    "The Jacobian here is diagonal becuase $\\frac{\\partial A^{(L)}_i}{\\partial P^{(L)}_j}$ is only defined for $i = j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of loss function w. r. t. the weights of a layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "    P^{(L)}_i =  B^{(L)}_i + \\sum_j W_{ij}^{(L)} A^{(L-1)}_j\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial W^{(L)}_{ij}} =& \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial P^{(L)}_i} \\frac{\\partial P^{(L)}_i}{\\partial W^{(L)}_{ij}}\\\\\n",
    "    =& \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial P^{(L)}_i} A^{(L-1)}_j\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{W^{(L)}} l\\left(y, f(x)\\right) = \\left(\\nabla_{P^{(L)}}{l\\left(y, f(x)\\right)}\\right) \\left(A^{(L-1)}\\right)^\\top\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of loss function w. r. t. the biases of a layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "    P^{(L)}_i =  B^{(L)}_i + \\sum_j W_{ij}^{(L)} A^{(L-1)}_j\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial B^{(L)}_i} =& \\frac{\\partial l\\left(y, f(x)\\right)}{\\partial A^{(L)}_i} \\frac{\\partial A^{(L)}_i}{\\partial B^{(L)}_i}\\\\\n",
    "    =& \\frac{\\partial l\\paren{y, f(x)}}{\\partial A^{(L)}_i}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{B^{(L)}}{l\\paren{y, f(x)}} = \\nabla_{A^{(L)}}{l\\left(y, f(x)\\right)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))\n",
    "sigmoid = np.vectorize(sigmoid)\n",
    "\n",
    "def sigmoid_(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "sigmoid_ = np.vectorize(sigmoid_)\n",
    "\n",
    "def relu(x):\n",
    "    return x if x > 0 else 0\n",
    "relu = np.vectorize(relu)\n",
    "\n",
    "def relu_(x):\n",
    "    return 1 if x > 0 else 0\n",
    "relu_ = np.vectorize(relu_)\n",
    "\n",
    "def softmax(x):\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def mse(error):\n",
    "    return np.sum(error**2)/len(error)\n",
    "# mse = np.vectorize(mse)\n",
    "\n",
    "def d_mse_d_O(error):\n",
    "    \"\"\"\n",
    "    partial derivative of MSE w.r.t output layer activation\n",
    "    return value is proportional to actual derivative\n",
    "    \"\"\"\n",
    "    return error\n",
    "d_mse_d_O = np.vectorize(d_mse_d_O)\n",
    "    \n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, dims, acfs, acfs_deriv, loss_fnc, loss_fnc_deriv):\n",
    "        self.layers_acs = [np.zeros(shape=d) for d in dims]\n",
    "        self.layers_preacs = [np.zeros(shape=d) for d in dims]\n",
    "        self.biases = [np.random.normal(size=d) for d in dims]\n",
    "        self.acfs = acfs\n",
    "        # have a dummy entry so that layers are co-index with their weights\n",
    "        self.weights = [None]+[np.random.normal(size=(d2, d1)) for d1, d2 in zip(dims[:-1], dims[1:])]\n",
    "        self.acfs_deriv = acfs_deriv\n",
    "        self.loss_fnc_deriv = loss_fnc_deriv\n",
    "        self.loss_fnc = loss_fnc\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # input layer has no activation function, thus place input\n",
    "        # directly into self.layers_acs[0]\n",
    "        self.fprop(x)\n",
    "        return self.layers_acs[-1]\n",
    "    \n",
    "    def show(self):\n",
    "        for i, (w, l) in enumerate(zip(self.weights[1:], self.layers_acs)):\n",
    "            print(f'l{i}')\n",
    "            print(l)\n",
    "        print(f'l{i+1}')\n",
    "        print(self.layers_acs[-1])\n",
    "        print()\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        self.layers_acs[0] = x\n",
    "                                                            # skip the dummy entry\n",
    "        for i, (weights, acf) in enumerate(zip(self.weights[1:], self.acfs)):\n",
    "            preacs = np.dot(weights, self.layers_acs[i])\n",
    "            self.layers_preacs[i+1] = preacs\n",
    "            acs = acf(preacs)\n",
    "            self.layers_acs[i+1] = acs\n",
    "            \n",
    "    def train(self, x : np.array, y_true_probs : np.array):\n",
    "        \"\"\"\n",
    "        x : input layer activations\n",
    "        y : vector of target activations\n",
    "        \"\"\"\n",
    "        y_pred_probs = self.predict(x)\n",
    "        error = y_true_probs - y_pred_probs\n",
    "        return self.bprop(error), self.loss_fnc(error)\n",
    "    \n",
    "    def bprop(self, error):\n",
    "        # define containers to hold gradient values\n",
    "        layer_grads = [np.zeros_like(l) for l in self.layers_acs]\n",
    "        weight_grads = [np.zeros_like(w) for w in self.weights]\n",
    "        bias_grads = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # assign gradient with respect to output activation\n",
    "        layer_grads[-1] = self.loss_fnc_deriv(error)\n",
    "        # assign gradient with respect to output pre-activation\n",
    "        layer_grads[-1] *= (self.acfs_deriv[-1](self.layers_preacs[-1]))\n",
    "        \n",
    "        for i in range(len(self.weights)-1, 0, -1):\n",
    "            # assign gradient with respect to the weights of layer i\n",
    "            weight_grads[i] = np.dot(layer_grads[i].reshape(-1,1),\n",
    "                                        self.layers_acs[i-1].T.reshape(1,-1))\n",
    "            # assign gradient with respect to the biases of layer i\n",
    "            bias_grads[i] = layer_grads[i]\n",
    "            #assign gradient with respect to the activations of layer i-1\n",
    "            layer_grads[i-1] = np.dot(weight_grads[i].T, layer_grads[i])\n",
    "            #assign gradient with respect to the pre-activations of layer i-1\n",
    "            layer_grads[i-1] = (layer_grads[i-1] * self.acfs_deriv[i-1](self.layers_preacs[i-1]))\n",
    "    \n",
    "        return {'weight_grads' : weight_grads[1:],\n",
    "                'bias_grads' : bias_grads}\n",
    "        \n",
    "\n",
    "def index2onehot(indices):\n",
    "    n = np.max(indices)\n",
    "    for i in indices:\n",
    "        v = np.zeros(n+1)\n",
    "        v[i] = 1\n",
    "        yield v\n",
    "        \n",
    "def train(model, X, Y, batchsize=128, epochs=1, lrate=.1, decay=1.001):\n",
    "            \n",
    "    from itertools import zip_longest\n",
    "    def batcher(iterable, n, fillvalue=None):\n",
    "        args = [iter(iterable)] * n\n",
    "        return zip_longest(*args, fillvalue=fillvalue)\n",
    "    \n",
    "    for epochi in range(epochs):\n",
    "        print('epoch no.', epochi+1)\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(X)//batchsize\n",
    "        for batchi, batch in enumerate(batcher(zip(X, Y), batchsize)):\n",
    "            print(\"{:0.4f}%\".format(((batchi+1)/num_batches)*100), end='\\r')\n",
    "            weight_grads_acc = [np.zeros_like(w) for w in model.weights[1:]]\n",
    "            bias_grads_acc = [np.zeros_like(b) for b in model.biases]\n",
    "            batch_loss = 0\n",
    "            for pair in batch:\n",
    "                if pair is not None:\n",
    "                    x = pair[0]\n",
    "                    y = pair[1]\n",
    "                    gradients, loss = model.train(x, y)\n",
    "                    for wga, wg in zip(weight_grads_acc, gradients['weight_grads']):\n",
    "                        wga += wg\n",
    "                    for bga, bg in zip(bias_grads_acc, gradients['bias_grads']):\n",
    "                        bga += bg\n",
    "                    batch_loss += loss\n",
    "            for i, wga in enumerate(weight_grads_acc):\n",
    "                model.weights[i+1] -= lrate * (wga / batchsize)\n",
    "            for i, bga in enumerate(bias_grads_acc):\n",
    "                model.biases[i] -= lrate * (bga / batchsize)\n",
    "            epoch_loss+=batch_loss\n",
    "        lrate/=decay\n",
    "        print('epoch loss:', epoch_loss/num_batches)\n",
    "        print('epoch acc:', eval(model, X, Y))\n",
    "        \n",
    "\n",
    "def eval(model, X, Y):\n",
    "    preds = []\n",
    "    for x, y in zip(X, Y):\n",
    "        y_pred = np.argmax(model.predict(x))\n",
    "        preds.append(y_pred)\n",
    "    preds = np.array(preds)\n",
    "    return np.sum(np.apply_along_axis(np.argmax, 1, Y) == preds)/len(Y)\n",
    "\n",
    "def eval_regress(model, X, Y):\n",
    "    preds = []\n",
    "    for x, y in zip(X, Y):\n",
    "        y_pred = model.predict(x)[0]\n",
    "        preds.append(y_pred)\n",
    "    preds = np.array(preds)\n",
    "    return np.sum((Y - preds)**2)/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = load_digits()\n",
    "targets = mnist.target\n",
    "data = mnist.data\n",
    "\n",
    "X_train = data\n",
    "X_train = X_train / 255\n",
    "Y_train = targets\n",
    "num_pixels = X_train[0].shape[0]\n",
    "Y_train = np.array(list(index2onehot(Y_train)))\n",
    "num_classes = Y_train.shape[1]\n",
    "\n",
    "mlp = MLP([64,10,10], [relu, sigmoid], [relu_, sigmoid_], mse, d_mse_d_O)\n",
    "train(mlp, X_train, Y_train, lrate=1, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
