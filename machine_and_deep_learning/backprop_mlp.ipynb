{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation for educational purposes of an MLP and a back propagation with a basic optimization rule. The frist few cells derive the parameter updates formally. Then the implementation follows that uses these equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\paren}[1]{\\mathopen{}\\left( {#1}_{{}_{}}\\,\\negthickspace\\right)\\mathclose{}}\n",
    "\\newcommand{\\pda}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\pdb}[2]{\\frac{\\partial}{\\partial #2} #1}\n",
    "\\newcommand{\\grad}[2]{\\nabla_{#2}{#1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gradient of loss function w. r. t. the activation of the output layer:}$\n",
    "\n",
    "Let the output layer activation be the sigmoid function and let the loss function be the mean square error.\\\\\n",
    "\n",
    "\\begin{align}\n",
    "    \\pda{l\\paren{y, f(x)}}{A^{(L)}_i} ={}& \\pda{l\\paren{y, f(x)}}{f (x)_i}\\\\\n",
    "    & \\text{by definition}\\\\\n",
    "    ={}& \\pdb{\\frac{1}{2n} \\sum_{j=1}^n \\paren{y_j - f(x)_j}^2}{f(x)}\\\\\n",
    "    ={}& \\frac{1}{2n} \\sum_{j=1}^n \\pda{\\paren{y_j - f(x)_j^2}}{f(x)_i}\\\\\n",
    "    ={}& \\frac{1}{2n} 2 \\paren{y_i - f(x)_i}\\\\\n",
    "    ={}& \\frac{1}{n} \\paren{y_i - f(x)_i}\\\\ \n",
    "    \\propto{}& y_i - f(x)_i\\label{eq1}\n",
    "    % & \\text{\\textcolor{gray}{for the case of $g^{(L)} = \\sigma$}}\\\\\n",
    "    % ={}& \\textcolor{gray}{(y - \\sigma(P^{(L)})_i)}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{A^{(L)}}{l\\paren{y, f(x)}} \\propto y - f(x)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $f$ is the function computed by the network and $g^{(L)}$ is the activation function of layer $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gradient of loss function, w. r. t. the pre-activation of the output layer:}$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\pda{l\\paren{y, f(x)}}{P^{(L)}_i} ={}& \\pda{l\\paren{y, f(x)}}{A^{(L)}_i} \\pda{A^{(L)}_i}{P^{(L)}_i}\\\\\n",
    "    \\propto {}& \\paren{y_i - f(x)_i} \\pda{A^{(L)}_i}{P^{(L)}_i}\\\\\n",
    "    ={}& \\paren{y_i - f(x)_i} \\pda{g^{(L)}\\paren{P^{(L)}}_i}{P^{(L)}_i}\\\\\n",
    "    ={}& \\paren{y_i - f(x)_i} g'^{(L)}\\paren{P^{(L)}}_i\\\\\n",
    "    % & \\text{\\textcolor{gray}{for the case of $g^{(L)} = \\sigma$}}\\\\\n",
    "    % ={}&\\textcolor{gray}{(y - \\sigma(P^{(L)})_i) \\sigma(P^{(L)})_i(1 - \\sigma(P^{(L)})_i)}\n",
    "\\end{align}\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{P^{(L)}} l\\paren{y, f(x)} \\propto \\paren{y - f(x)} \\otimes g'^{(L)}(P^{(L)})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gradient of loss function w. r. t. the activation of a hidden layer:}$\n",
    "\n",
    "\\begin{equation*}\n",
    "    P^{(L+1)}_i =  B^{(L+1)}_i + \\sum_j W_{ij}^{(L+1)} A^{(L)}_j\n",
    "\\end{equation*}\n",
    "\\begin{align}\n",
    "    \\pda{l\\paren{y, f(x)}}{A^{(L)}_i} ={}& \\sum_i \\pda{\\paren{(y, f(x)}}{P^{(L+1)}_i} \\pda{P^{(L+1)}_i}{A^{(L)}_i}\\\\\n",
    "    ={}& \\sum_i \\pda{l\\paren{y, f(x)}}{P^{(L+1)}_i} W_{ij}^{(L+1)}\\\\\n",
    "    ={}& \\left( W^{(L+1)}_{\\cdot j} \\right)^\\top  \\nabla_{P^{(L+1)}} l\\paren{y, f(x)}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{A^{(L)}} l\\paren{y, f(x)} = \\left( W^{(L+1)} \\right)^\\top  \\nabla_{P^{(L+1)}} l\\paren{y, f(x)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gradient of loss function w.\\ r.\\ t.\\ the pre-activation of a hidden layer:}$\n",
    "\n",
    "\\begin{equation*}\n",
    "    A^{(L)}_i =  g^{(L)} \\left( P^{(L)} \\right)_i\n",
    "\\end{equation*}\n",
    "\\begin{align}\n",
    "    \\pda{l\\paren{y, f(x)}}{P^{(L)}_i} ={}& \\pda{l\\paren{y, f(x)}}{A^{(L)}_i} \\pda{A^{(L)}_i}{P^{(L)}_i}\\\\\n",
    "    ={}& \\pda{l\\paren{y, f(x)}}{A^{(L)}_i} g'^{(L)} \\left( P^{(L)} \\right)_i\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{P^{(L)}} l\\paren{y, f(x)} = \\paren{\\nabla_{A^{(L)}} l\\paren{y, f(x)}}^\\top \\underbrace{\\nabla_{P^{(L)}} A^{(L)}}_{\\text{Jacobian}}\n",
    "\\end{equation*}\n",
    "The Jacobian here is diagonal becuase $\\pda{A^{(L)}_i}{P^{(L)}_j}$ is only defined for $i = j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gradient of loss function w. r. t. the weights of a layer:}$\n",
    "\n",
    "\\begin{equation*}\n",
    "    P^{(L)}_i =  B^{(L)}_i + \\sum_j W_{ij}^{(L)} A^{(L-1)}_j\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{align}\n",
    "    \\pda{l\\paren{y, f(x)}}{W^{(L)}_{ij}} ={}& \\pda{l\\paren{y, f(x)}}{P^{(L)}_i} \\pda{P^{(L)}_i}{W^{(L)}_{ij}}\\\\\n",
    "    ={}& \\pda{l\\paren{y, f(x)}}{P^{(L)}_i} A^{(L-1)}_j\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\nabla_{W^{(L)}} l\\paren{y, f(x)} = \\paren{\\grad{l\\paren{y, f(x)}}{P^{(L)}}} \\paren{A^{(L-1)}}^\\top\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gradient of loss function w. r. t. the biases of a layer:}$\n",
    "\n",
    "\\begin{equation*}\n",
    "    P^{(L)}_i =  B^{(L)}_i + \\sum_j W_{ij}^{(L)} A^{(L-1)}_j\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{align}\n",
    "    \\pda{l\\paren{y, f(x)}}{B^{(L)}_i} ={}& \\pda{l\\paren{y, f(x)}}{A^{(L)}_i} \\pda{A^{(L)}_i}{B^{(L)}_i}\\\\\n",
    "    ={}& \\pda{l\\paren{y, f(x)}}{A^{(L)}_i}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\implies \\grad{l\\paren{y, f(x)}}{B^{(L)}} = \\grad{l\\paren{y, f(x)}}{A^{(L)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-9869bfa792ce>\", line 3, in <module>\n",
      "    sigmoid = np.vectorize(sigmoid)\n",
      "NameError: name 'np' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/lo-pilno/miniconda3/envs/normal/lib/python3.7/posixpath.py\", line 383, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))\n",
    "sigmoid = np.vectorize(sigmoid)\n",
    "\n",
    "def sigmoid_(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "sigmoid_ = np.vectorize(sigmoid_)\n",
    "\n",
    "def relu(x):\n",
    "    return x if x > 0 else 0\n",
    "relu = np.vectorize(relu)\n",
    "\n",
    "def relu_(x):\n",
    "    return 1 if x > 0 else 0\n",
    "relu_ = np.vectorize(relu_)\n",
    "\n",
    "def softmax(x):\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def mse(error):\n",
    "    return np.sum(error**2)/len(error)\n",
    "# mse = np.vectorize(mse)\n",
    "\n",
    "def d_mse_d_O(error):\n",
    "    \"\"\"\n",
    "    partial derivative of MSE w.r.t output layer activation\n",
    "    return value is proportional to actual derivative\n",
    "    \"\"\"\n",
    "    return error\n",
    "d_mse_d_O = np.vectorize(d_mse_d_O)\n",
    "    \n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, dims, acfs, acfs_deriv, loss_fnc, loss_fnc_deriv):\n",
    "        self.layers_acs = [np.zeros(shape=d) for d in dims]\n",
    "        self.layers_preacs = [np.zeros(shape=d) for d in dims]\n",
    "        self.biases = [np.random.normal(size=d) for d in dims]\n",
    "        self.acfs = acfs\n",
    "        # have a dummy entry so that layers are co-index with their weights\n",
    "        self.weights = [None]+[np.random.normal(size=(d2, d1)) for d1, d2 in zip(dims[:-1], dims[1:])]\n",
    "        self.acfs_deriv = acfs_deriv\n",
    "        self.loss_fnc_deriv = loss_fnc_deriv\n",
    "        self.loss_fnc = loss_fnc\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # input layer has no activation function, thus place input\n",
    "        # directly into self.layers_acs[0]\n",
    "        self.fprop(x)\n",
    "        return self.layers_acs[-1]\n",
    "    \n",
    "    def show(self):\n",
    "        for i, (w, l) in enumerate(zip(self.weights[1:], self.layers_acs)):\n",
    "            print(f'l{i}')\n",
    "            print(l)\n",
    "        print(f'l{i+1}')\n",
    "        print(self.layers_acs[-1])\n",
    "        print()\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        self.layers_acs[0] = x\n",
    "                                                            # skip the dummy entry\n",
    "        for i, (weights, acf) in enumerate(zip(self.weights[1:], self.acfs)):\n",
    "            preacs = np.dot(weights, self.layers_acs[i])\n",
    "            self.layers_preacs[i+1] = preacs\n",
    "            acs = acf(preacs)\n",
    "            self.layers_acs[i+1] = acs\n",
    "            \n",
    "    def train(self, x : np.array, y_true_probs : np.array):\n",
    "        \"\"\"\n",
    "        x : input layer activations\n",
    "        y : vector of target activations\n",
    "        \"\"\"\n",
    "        y_pred_probs = self.predict(x)\n",
    "        error = y_true_probs - y_pred_probs\n",
    "        return self.bprop(error), self.loss_fnc(error)\n",
    "    \n",
    "    def bprop(self, error):\n",
    "        # define containers to hold gradient values\n",
    "        layer_grads = [np.zeros_like(l) for l in self.layers_acs]\n",
    "        weight_grads = [np.zeros_like(w) for w in self.weights]\n",
    "        bias_grads = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # assign gradient with respect to output activation\n",
    "        layer_grads[-1] = self.loss_fnc_deriv(error)\n",
    "        # assign gradient with respect to output pre-activation\n",
    "        layer_grads[-1] *= (self.acfs_deriv[-1](self.layers_preacs[-1]))\n",
    "        \n",
    "        for i in range(len(self.weights)-1, 0, -1):\n",
    "            # assign gradient with respect to the weights of layer i\n",
    "            weight_grads[i] = np.dot(layer_grads[i].reshape(-1,1),\n",
    "                                        self.layers_acs[i-1].T.reshape(1,-1))\n",
    "            # assign gradient with respect to the biases of layer i\n",
    "            bias_grads[i] = layer_grads[i]\n",
    "            #assign gradient with respect to the activations of layer i-1\n",
    "            layer_grads[i-1] = np.dot(weight_grads[i].T, layer_grads[i])\n",
    "            #assign gradient with respect to the pre-activations of layer i-1\n",
    "            layer_grads[i-1] = (layer_grads[i-1] * self.acfs_deriv[i-1](self.layers_preacs[i-1]))\n",
    "    \n",
    "        return {'weight_grads' : weight_grads[1:],\n",
    "                'bias_grads' : bias_grads}\n",
    "        \n",
    "\n",
    "def index2onehot(indices):\n",
    "    n = np.max(indices)\n",
    "    for i in indices:\n",
    "        v = np.zeros(n+1)\n",
    "        v[i] = 1\n",
    "        yield v\n",
    "        \n",
    "def train(model, X, Y, batchsize=128, epochs=1, lrate=.1, decay=1.001):\n",
    "            \n",
    "    from itertools import zip_longest\n",
    "    def batcher(iterable, n, fillvalue=None):\n",
    "        args = [iter(iterable)] * n\n",
    "        return zip_longest(*args, fillvalue=fillvalue)\n",
    "    \n",
    "    for epochi in range(epochs):\n",
    "        print('epoch no.', epochi+1)\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(X)//batchsize\n",
    "        for batchi, batch in enumerate(batcher(zip(X, Y), batchsize)):\n",
    "            print(\"{:0.4f}%\".format(((batchi+1)/num_batches)*100), end='\\r')\n",
    "            weight_grads_acc = [np.zeros_like(w) for w in model.weights[1:]]\n",
    "            bias_grads_acc = [np.zeros_like(b) for b in model.biases]\n",
    "            batch_loss = 0\n",
    "            for pair in batch:\n",
    "                if pair is not None:\n",
    "                    x = pair[0]\n",
    "                    y = pair[1]\n",
    "                    gradients, loss = model.train(x, y)\n",
    "                    for wga, wg in zip(weight_grads_acc, gradients['weight_grads']):\n",
    "                        wga += wg\n",
    "                    for bga, bg in zip(bias_grads_acc, gradients['bias_grads']):\n",
    "                        bga += bg\n",
    "                    batch_loss += loss\n",
    "            for i, wga in enumerate(weight_grads_acc):\n",
    "                model.weights[i+1] -= lrate * (wga / batchsize)\n",
    "            for i, bga in enumerate(bias_grads_acc):\n",
    "                model.biases[i] -= lrate * (bga / batchsize)\n",
    "            epoch_loss+=batch_loss\n",
    "        lrate/=decay\n",
    "        print('epoch loss:', epoch_loss/num_batches)\n",
    "        print('epoch acc:', eval(model, X, Y))\n",
    "        \n",
    "\n",
    "def eval(model, X, Y):\n",
    "    preds = []\n",
    "    for x, y in zip(X, Y):\n",
    "        y_pred = np.argmax(model.predict(x))\n",
    "        preds.append(y_pred)\n",
    "    preds = np.array(preds)\n",
    "    return np.sum(np.apply_along_axis(np.argmax, 1, Y) == preds)/len(Y)\n",
    "\n",
    "def eval_regress(model, X, Y):\n",
    "    preds = []\n",
    "    for x, y in zip(X, Y):\n",
    "        y_pred = model.predict(x)[0]\n",
    "        preds.append(y_pred)\n",
    "    preds = np.array(preds)\n",
    "    return np.sum((Y - preds)**2)/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = load_digits()\n",
    "targets = mnist.target\n",
    "data = mnist.data\n",
    "\n",
    "X_train = data\n",
    "X_train = X_train / 255\n",
    "Y_train = targets\n",
    "num_pixels = X_train[0].shape[0]\n",
    "Y_train = np.array(list(index2onehot(Y_train)))\n",
    "num_classes = Y_train.shape[1]\n",
    "\n",
    "mlp = MLP([64,10,10], [relu, sigmoid], [relu_, sigmoid_], mse, d_mse_d_O)\n",
    "train(mlp, X_train, Y_train, lrate=1, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
